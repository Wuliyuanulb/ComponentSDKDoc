# RunSettings
Component.runsettings is used to specify run level settings, including below aspects:

- target: compute target
- environment_variable: user can pass in dictionary to set enrivonment variables
- resource_layout: gpu/cpu/mem ask and elastic training setting for job
- parallel/hdinsight: component type related run settings

## User Interface
The example of using runsettings in dsl pipeline:
```python
# specify target
component.runsettings.target = "aml-compute"

# optionally specify static settings like resource 
component.runsettings.resource_layout.node_count = 2
component.runsettings.resource_layout.process_count_per_node = 2

# optionally specify component type dynamic setting, like parallel component
component.runsettings.parallel.error_threshold = 10
component.runsettings.parallel.mini_batch_size = 200
```
You can also set runsettings using configure function:
```python
# specify target
component.runsettings.configure(target="aml-compute")

# optionally specify static settings like resource 
component.runsettings.resource_layout.configure(node_count=2, process_count_per_node=2)

# optionally specify component type dynamic setting, like parallel component
component.runsettings.parallel.configure(error_threshold=10, mini_batch_size=200)
```

## RunSettings Reference
Below are the reference of full list of configuration sections available in Runsettings.

### target
Target refers to the compute where the job is scheduled for execution.

Target is a string which should be the name of a valid compute in user workspace. Target might also be arm resource id in sdk vnext.
```python
component.runsettings.target = "aml-compute"
```

### environment_variables
environment_variables can be used to specify envrionment variables to be passed. It is a dictionary of environment name to environment value mapping. User can use this to adjust some component runtime behavior which is not exposed as component parameter, e.g. enable some debug switch.
> **Note**:
HDInsight Component doesn't support this, you can use `hdinsight.conf` as a replacement.

```python
component.runsetting.environment_variables = {'EXAMPLE_ENV_VAR': 'example_value'}
```

### resource_layout
Resource section controls the number of nodes, cpus, gpus the job will consume.

| Name | Type | Required | Default value | Description |
|-|-|-|-|-|
| node_count | Int | Yes | - | Number of nodes in the compute target used for running the Distributed Component. |
| process_count_per_node | Int | No | Number of cores on node | Number of processes executed on each node. |

#### Examples
```python
component.runsettings.resource_layout.configure(
    node_count=2,
    process_count_per_node=2)
# or
component.runsettings.resource_layout.node_count = 2
component.runsettings.resource_layout.process_count_per_node = 2
```

### parallel
This section contains specific settings for [Parallel Component](../components/parallel_component.md).

| Name | Type | Required | Default value | Description |
|-|-|-|-|-|
| node_count | Int | Yes | - | Number of nodes in the compute target used for running the Parallel Component. |
| process_count_per_node | Int | No | Number of cores on node | Number of processes executed on each node. |
| error_threshold | Int | No | -1 | The number of file failures for the input FileDataset that should be ignored during processing. If the error count goes above this value, then the job will be aborted. Error threshold is for the entire input and not for individual mini-batches sent to run() method. The range is [-1, int.max]. -1 indicates ignoring all failures during processing. |
| mini_batch_size | String | No | 10 | For the FileDataset input, this field is the number of files user script can process in one run() call. Example values are 64, 1024, 65536 and 1048576. |
| logging_level | String | No | INFO | A string of the logging level name, which is defined in 'logging'. Possible values are 'WARNING', 'INFO', and 'DEBUG'. |
| run_invocation_timeout | Int | No | 60 | Timeout in seconds for each invocation of the run() method. |
| run_max_try | Int | No | 3 | The number of maximum tries for a failed or timeout mini batch. A mini batch with dequeue count greater than this won't be processed again and will be deleted directly. |
| partition_keys | JsonString or Dictionary | No | None | Please refer to  [PRS docs](https://github.com/Azure/PRSPreviewFeatures) for more details. |
| version | String | No | v1 | Please refer to  [PRS docs](https://github.com/Azure/PRSPreviewFeatures) for more details. |

> For more questions on ParallelComponent, please contact [PRS team](mailto:amlbiteam@microsoft.com).

#### Examples
```python
component.runsettings.parallel.configure(
    error_threshold=-1,
    mini_batch_size=10,
    logging_level="INFO",
    run_invocation_timeout=60,
    run_max_try=3)
# or
component.runsettings.parallel.error_threshold = -1
component.runsettings.parallel.mini_batch_size = 10
```

### hdinsight
This section contains specific settings for [HDInsight Component](../components/hdi_component.md).

| Name | Type | Required | Description |
|-|-|-|-|
| queue | String | No | The name of the YARN queue to which submitted. |
| driver_memory | String | No | Amount of memory to use for the driver process. It's the same format as JVM memory strings. Use lower-case suffixes, e.g. k, m, g, t, and p, for kibi-, mebi-, gibi-, tebi-, and pebibytes, respectively. Example values are 10k, 10m and 10g. |
| driver_cores | Int | No | Number of cores to use for the driver process. |
| executor_memory | String | No | Amount of memory to use per executor process. It's the same format as JVM memory strings. Use lower-case suffixes, e.g. k, m, g, t, and p, for kibi-, mebi-, gibi-, tebi-, and pebibytes, respectively. |
| executor_cores | Int | No | Number of cores to use for each executor. |
| number_executors | Int | No | Number of executors to launch for this session. |
| conf | Dictionary<String, String> or JsonString | No | Spark configuration properties. |
| name | String | No | The name of this session. |

> Please refer to [spark docs](https://spark.apache.org/docs/latest/configuration.html) for default values of some fields.

#### Examples
```python
component.runsettings.hdinsight.configure(
    name="session_name",
    queue="default",
    driver_memory="1g",
    driver_cores=4,
    executor_memory="4g",
    executor_cores=4,
    number_executors=4,
    conf={
        "spark.yarn.maxAppAttempts": "1",
        "spark.yarn.appMasterEnv.PYSPARK_PYTHON": "/usr/bin/anaconda/envs/py35/bin/python3",
        "spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON": "/usr/bin/anaconda/envs/py35/bin/python3"
    }
)
# or
component.runsettings.hdinsight.name = "session_name"
component.runsettings.hdinsight.conf = {
        "spark.yarn.maxAppAttempts": "1",
        "spark.yarn.appMasterEnv.PYSPARK_PYTHON": "/usr/bin/anaconda/envs/py35/bin/python3",
        "spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON": "/usr/bin/anaconda/envs/py35/bin/python3"
    }
```

#### FAQ
##### How to get the absolute path of py_files in code?
If you use Python, you can get it by this way:
```python
from pyspark.sql import SparkSession

# Get the spark session
spark = SparkSession.builder.getOrCreate()
spark_conf = spark.sparkContext.getConf()
py_files = spark_conf.get('spark.yarn.dist.pyFiles')
```
